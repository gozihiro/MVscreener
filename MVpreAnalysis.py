import os
import sys
import pandas as pd
from datetime import datetime, timedelta, timezone
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
import io

# --- ç’°å¢ƒå¤‰æ•°ã®å–å¾— ---
def get_env(name):
    return os.environ.get(name)

CLIENT_ID = get_env('CLIENT_ID')
CLIENT_SECRET = get_env('CLIENT_SECRET')
REFRESH_TOKEN = get_env('REFRESH_TOKEN')
PARENT_FOLDER_ID = get_env('GDRIVE_FOLDER_ID')

# è§£æžå¯¾è±¡ã®å…¨é …ç›®
REQUIRED_COLS = [
    'éŠ˜æŸ„', 'ä¾¡æ ¼', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æˆé•·æ€§åˆ¤å®š', 'å£²ä¸Šæˆé•·(%)', 
    'å–¶æ¥­åˆ©ç›Šæˆé•·(EBITDA)%', 'ç´”åˆ©ç›Šæˆé•·(%)', 'å–¶æ¥­CF(M)', 'æ™‚ä¾¡ç·é¡(B)'
]

def get_drive_service():
    creds = Credentials(token=None, refresh_token=REFRESH_TOKEN, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, token_uri="https://oauth2.googleapis.com/token")
    return build('drive', 'v3', credentials=creds)

def get_or_create_summary_folder(service):
    """ç¾åœ¨ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã«Summaryãƒ•ã‚©ãƒ«ãƒ€ã‚’ç‰¹å®šã¾ãŸã¯ä½œæˆï¼ˆé‡è¤‡é˜²æ­¢ã‚’æ¥µé™ã¾ã§å¼·åŒ–ï¼‰"""
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã ã‘ã«é ¼ã‚‰ãšã€è¦ªãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦Pythonå´ã§åå‰ã‚’å®Œå…¨ä¸€è‡´æ¯”è¼ƒã™ã‚‹
    query = f"'{PARENT_FOLDER_ID}' in parents and trashed = false"
    
    try:
        results = service.files().list(
            q=query, 
            fields="files(id, name, mimeType)",
            pageSize=1000
        ).execute()
        items = results.get('files', [])
        
        for item in items:
            # åå‰ã¨ãƒ•ã‚©ãƒ«ãƒ€å±žæ€§ãŒä¸€è‡´ã™ã‚‹æ—¢å­˜ã®ã‚‚ã®ã‚’æŽ¢ã™
            if item['name'] == 'Summary' and item['mimeType'] == 'application/vnd.google-apps.folder':
                print(f"ðŸ“ æ—¢å­˜ã® Summary ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç‰¹å®šã—ã¾ã—ãŸ (ID: {item['id']})")
                return item['id']
                
    except Exception as e:
        print(f"âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿æ–°è¦ä½œæˆ
    file_metadata = {
        'name': 'Summary',
        'mimeType': 'application/vnd.google-apps.folder',
        'parents': [PARENT_FOLDER_ID]
    }
    folder = service.files().create(body=file_metadata, fields='id').execute()
    print(f"ðŸ“ Summary ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")
    return folder.get('id')

def get_target_time_ranges():
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    ranges = []
    for i in range(7):
        day = now - timedelta(days=i)
        if day.weekday() in [1, 2, 3, 4, 5]: # ç«ã€œåœŸ(JST) = æœˆã€œé‡‘(Market)
            start = day.replace(hour=0, minute=0, second=0, microsecond=0)
            end = day.replace(hour=23, minute=59, second=59, microsecond=0)
            ranges.append((start, end))
        if len(ranges) == 5: break
    return sorted(ranges)

def fetch_weekly_data():
    service = get_drive_service()
    ranges = get_target_time_ranges()
    weekly_dfs = []
    market_metadatas = []

    print("=== Phase 1: ãƒ‡ãƒ¼ã‚¿åŽé›†ã¨å¸‚å ´ç’°å¢ƒè§£æž (JST) ===")
    for start, end in ranges:
        market_date = (start - timedelta(days=1)).strftime('%m/%d')
        query = (f"'{PARENT_FOLDER_ID}' in parents and name = 'minervini_final_results.csv' "
                 f"and createdTime >= '{start.isoformat()}' and createdTime <= '{end.isoformat()}' "
                 f"and trashed = false")
        
        res = service.files().list(q=query, fields="files(id, createdTime)", orderBy="createdTime").execute()
        files = res.get('files', [])

        if files:
            file_id = files[0]['id']
            req = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done: _, done = downloader.next_chunk()
            
            fh.seek(0)
            raw_data = fh.read().decode('utf-8-sig').splitlines()
            
            # 1è¡Œç›®ã®å¸‚å ´ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
            metadata_line = raw_data[0] if raw_data else "No Metadata"
            market_metadatas.append({'Date': market_date, 'Metadata': metadata_line})
            
            # 2è¡Œç›®ä»¥é™ã®éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(io.StringIO("\n".join(raw_data[1:])))
            for col in REQUIRED_COLS:
                if col not in df.columns: df[col] = "ä¸æ˜Ž"
            
            df = df[REQUIRED_COLS].copy()
            df['Date'] = market_date
            weekly_dfs.append(df)
            print(f"âœ… {market_date}: åŽé›†å®Œäº†")
        else:
            print(f"âŒ {market_date}: æœªæ¤œå‡º")

    return weekly_dfs, market_metadatas

def analyze_detailed_trend(dfs, metadatas):
    if not dfs: return None
    
    print("=== Phase 2: å¸‚å ´ãƒ»éŠ˜æŸ„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æž ===")
    all_raw = pd.concat(dfs, ignore_index=True)
    trend_df = all_raw.groupby('éŠ˜æŸ„').size().reset_index(name='å‡ºç¾å›žæ•°')
    
    # 1è¡Œç›®ã«æŒ¿å…¥ã™ã‚‹å¸‚å ´ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ç”¨ã®è¡Œ
    market_row = {'éŠ˜æŸ„': '### MARKET_ENVIRONMENT ###', 'å‡ºç¾å›žæ•°': '-'}

    for df, meta in zip(dfs, metadatas):
        date = meta['Date']
        # å¸‚å ´ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç­‰ã‚’æ—¥ä»˜ã”ã¨ã®ã€Œä¾¡æ ¼ã€åˆ—ã®å ´æ‰€ã«æ ¼ç´ã—ã¦è¡¨ç¤º
        market_row[f'ä¾¡æ ¼_{date}'] = meta['Metadata']
        
        # éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        daily_data = df.set_index('éŠ˜æŸ„').drop(columns=['Date']).add_suffix(f'_{date}')
        trend_df = trend_df.merge(daily_data, on='éŠ˜æŸ„', how='left')

    # å¸‚å ´ç’°å¢ƒã‚’æœ€ä¸Šéƒ¨ã«è¿½åŠ 
    result = pd.concat([pd.DataFrame([market_row]), trend_df], ignore_index=True)

    # å‡ºç¾å›žæ•°ã¨æœ€æ–°æ—¥ã®å£²ä¸Šæˆé•·ã§ã‚½ãƒ¼ãƒˆ
    latest_date = dfs[-1]['Date'].iloc[0] if isinstance(dfs[-1]['Date'], pd.Series) else dfs[-1]['Date']
    sort_col = f'å£²ä¸Šæˆé•·(%)_{latest_date}'
    
    if sort_col in result.columns:
        result['_sort'] = pd.to_numeric(result[sort_col], errors='coerce').fillna(-999)
        market_header = result.iloc[:1]
        stock_data = result.iloc[1:].sort_values(by=['å‡ºç¾å›žæ•°', '_sort'], ascending=False)
        result = pd.concat([market_header, stock_data]).drop(columns=['_sort'])
    
    return result.fillna('ï¼')

def upload_result_to_drive(file_path):
    service = get_drive_service()
    summary_folder_id = get_or_create_summary_folder(service)
    
    file_name = f"weekly_detailed_trend_{datetime.now().strftime('%Y%m%d')}.csv"
    file_metadata = {'name': file_name, 'parents': [summary_folder_id]}
    media = MediaFileUpload(file_path, mimetype='text/csv')
    
    query = f"'{summary_folder_id}' in parents and name = '{file_name}' and trashed = false"
    res = service.files().list(q=query).execute()
    files = res.get('files', [])

    if files:
        service.files().update(fileId=files[0]['id'], media_body=media).execute()
        print(f"âœ… Summary ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {file_name}")
    else:
        service.files().create(body=file_metadata, media_body=media).execute()
        print(f"âœ… Summary ãƒ•ã‚©ãƒ«ãƒ€ã«æ–°è¦ä¿å­˜ã—ã¾ã—ãŸ: {file_name}")

if __name__ == "__main__":
    if not all([CLIENT_ID, CLIENT_SECRET, REFRESH_TOKEN, PARENT_FOLDER_ID]):
        print("âŒ èªè¨¼æƒ…å ±ä¸è¶³")
        sys.exit(1)

    weekly_data, metadatas = fetch_weekly_data()
    if weekly_data:
        trend_result = analyze_detailed_trend(weekly_data, metadatas)
        if trend_result is not None:
            output_file = "weekly_detailed_trend.csv"
            trend_result.to_csv(output_file, index=False, encoding='utf-8-sig')
            upload_result_to_drive(output_file)
    else:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—")
