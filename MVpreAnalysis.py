import os
import sys
import pandas as pd
from datetime import datetime, timedelta, timezone
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
import io

# --- ç’°å¢ƒå¤‰æ•°ã®å–å¾— ---
def get_env(name):
    return os.environ.get(name)

CLIENT_ID = get_env('CLIENT_ID')
CLIENT_SECRET = get_env('CLIENT_SECRET')
REFRESH_TOKEN = get_env('REFRESH_TOKEN')
PARENT_FOLDER_ID = get_env('GDRIVE_FOLDER_ID')

# è§£æžå¯¾è±¡ã®å…¨é …ç›®
REQUIRED_COLS = [
    'éŠ˜æŸ„', 'ä¾¡æ ¼', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æˆé•·æ€§åˆ¤å®š', 'å£²ä¸Šæˆé•·(%)', 
    'å–¶æ¥­åˆ©ç›Šæˆé•·(EBITDA)%', 'ç´”åˆ©ç›Šæˆé•·(%)', 'å–¶æ¥­CF(M)', 'æ™‚ä¾¡ç·é¡(B)'
]

def get_drive_service():
    creds = Credentials(token=None, refresh_token=REFRESH_TOKEN, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, token_uri="https://oauth2.googleapis.com/token")
    return build('drive', 'v3', credentials=creds)

def get_or_create_summary_folder(service):
    """ç¾åœ¨ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã«Summaryãƒ•ã‚©ãƒ«ãƒ€ã‚’ç‰¹å®šã¾ãŸã¯ä½œæˆ"""
    query = f"'{PARENT_FOLDER_ID}' in parents and name = 'Summary' and mimeType = 'application/vnd.google-apps.folder' and trashed = false"
    res = service.files().list(q=query, fields="files(id)").execute()
    files = res.get('files', [])
    
    if files:
        return files[0]['id']
    else:
        file_metadata = {
            'name': 'Summary',
            'mimeType': 'application/vnd.google-apps.folder',
            'parents': [PARENT_FOLDER_ID]
        }
        folder = service.files().create(body=file_metadata, fields='id').execute()
        print(f"ðŸ“ Summaryãƒ•ã‚©ãƒ«ãƒ€ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")
        return folder.get('id')

def get_target_time_ranges():
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    ranges = []
    for i in range(7):
        day = now - timedelta(days=i)
        if day.weekday() in [1, 2, 3, 4, 5]: # ç«ã€œåœŸ(JST) = æœˆã€œé‡‘(Market)
            start = day.replace(hour=0, minute=0, second=0, microsecond=0)
            end = day.replace(hour=23, minute=59, second=59, microsecond=0)
            ranges.append((start, end))
        if len(ranges) == 5: break
    return sorted(ranges)

def fetch_weekly_data():
    service = get_drive_service()
    ranges = get_target_time_ranges()
    weekly_dfs = []
    market_metadatas = []

    print("=== Phase 1: ãƒ‡ãƒ¼ã‚¿åŽé›†ã¨å¸‚å ´ç’°å¢ƒè§£æž (JST) ===")
    for start, end in ranges:
        market_date = (start - timedelta(days=1)).strftime('%m/%d')
        q = f"'{PARENT_FOLDER_ID}' in parents and name = 'minervini_final_results.csv' and createdTime >= '{start.isoformat()}' and createdTime <= '{end.isoformat()}' and trashed = false"
        res = service.files().list(q=q, fields="files(id, createdTime)", orderBy="createdTime").execute()
        files = res.get('files', [])

        if files:
            file_id = files[0]['id']
            req = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done: _, done = downloader.next_chunk()
            
            fh.seek(0)
            content = fh.read().decode('utf-8-sig').splitlines()
            
            # 1è¡Œç›®ã®å¸‚å ´ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
            metadata_line = content[0] if content else ""
            market_metadatas.append({'Date': market_date, 'Metadata': metadata_line})
            
            # ãƒ‡ãƒ¼ã‚¿æœ¬ä½“ã‚’DFåŒ–
            df = pd.read_csv(io.StringIO("\n".join(content[1:])))
            for col in REQUIRED_COLS:
                if col not in df.columns: df[col] = "ä¸æ˜Ž"
            
            df = df[REQUIRED_COLS].copy()
            df['Date'] = market_date
            weekly_dfs.append(df)
            print(f"âœ… {market_date}: åŽé›†å®Œäº† (å¸‚å ´ãƒ‡ãƒ¼ã‚¿å«ã‚€)")
        else:
            print(f"âŒ {market_date}: æœªæ¤œå‡º")

    return weekly_dfs, market_metadatas

def analyze_detailed_trend(dfs, metadatas):
    if not dfs: return None
    
    print("=== Phase 2: å¸‚å ´ãƒ»éŠ˜æŸ„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æž ===")
    # å…¨æ—¥ç¨‹ã‚’ãƒžãƒ¼ã‚¸
    all_raw = pd.concat(dfs, ignore_index=True)
    
    # éŠ˜æŸ„ã”ã¨ã®å‡ºç¾å›žæ•°
    summary = all_raw.groupby('éŠ˜æŸ„').size().reset_index(name='å‡ºç¾å›žæ•°')
    
    # æ¨ªæŒã¡ï¼ˆPivotï¼‰å½¢å¼ã§ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ä½œæˆ
    trend_df = summary.copy()
    
    # å¸‚å ´ç’°å¢ƒè¡Œã®åˆæœŸåŒ–
    market_row = {'éŠ˜æŸ„': '### MARKET_ENVIRONMENT ###', 'å‡ºç¾å›žæ•°': '-'}

    # åŽé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜é †ã«å‡¦ç†
    for df, meta in zip(dfs, metadatas):
        date = meta['Date']
        # å¸‚å ´ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ä¾¡æ ¼åˆ—ã®ä½ç½®ã«æ ¼ç´
        market_row[f'ä¾¡æ ¼_{date}'] = meta['Metadata']
        
        # éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã®åˆ—ã‚’æ—¥ä»˜ä»˜ãã§çµåˆ
        daily_part = df.set_index('éŠ˜æŸ„').drop(columns=['Date']).add_suffix(f'_{date}')
        trend_df = trend_df.merge(daily_part, on='éŠ˜æŸ„', how='left')

    # å¸‚å ´ç’°å¢ƒè¡Œã‚’1è¡Œç›®ã«æŒ¿å…¥
    result = pd.concat([pd.DataFrame([market_row]), trend_df], ignore_index=True)

    # å‡ºç¾å›žæ•°ã¨æœ€æ–°ã®å£²ä¸Šæˆé•·ã§ã‚½ãƒ¼ãƒˆ
    latest_date = dfs[-1]['Date'].iloc[0] if isinstance(dfs[-1]['Date'], pd.Series) else dfs[-1]['Date']
    sort_col = f'å£²ä¸Šæˆé•·(%)_{latest_date}'
    
    if sort_col in result.columns:
        result['_sort'] = pd.to_numeric(result[sort_col], errors='coerce').fillna(-999)
        top = result.iloc[:1] # å¸‚å ´ç’°å¢ƒè¡Œ
        others = result.iloc[1:].sort_values(by=['å‡ºç¾å›žæ•°', '_sort'], ascending=False)
        result = pd.concat([top, others]).drop(columns=['_sort'])
    
    return result.fillna('ï¼')

def upload_result_to_drive(file_path):
    service = get_drive_service()
    summary_folder_id = get_or_create_summary_folder(service)
    
    file_name = f"weekly_detailed_trend_{datetime.now().strftime('%Y%m%d')}.csv"
    file_metadata = {'name': file_name, 'parents': [summary_folder_id]}
    media = MediaFileUpload(file_path, mimetype='text/csv')
    
    query = f"'{summary_folder_id}' in parents and name = '{file_name}' and trashed = false"
    res = service.files().list(q=query).execute()
    files = res.get('files', [])

    if files:
        service.files().update(fileId=files[0]['id'], media_body=media).execute()
        print(f"âœ… Summaryãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {file_name}")
    else:
        service.files().create(body=file_metadata, media_body=media).execute()
        print(f"âœ… Summaryãƒ•ã‚©ãƒ«ãƒ€ã«æ–°è¦ä¿å­˜ã—ã¾ã—ãŸ: {file_name}")

if __name__ == "__main__":
    if not all([CLIENT_ID, CLIENT_SECRET, REFRESH_TOKEN, PARENT_FOLDER_ID]):
        print("âŒ èªè¨¼æƒ…å ±ã®ä¸è¶³ã€‚GitHub Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    weekly_data, metadatas = fetch_weekly_data()
    if weekly_data:
        trend_result = analyze_detailed_trend(weekly_data, metadatas)
        if trend_result is not None:
            output_file = "weekly_detailed_trend.csv"
            trend_result.to_csv(output_file, index=False, encoding='utf-8-sig')
            upload_result_to_drive(output_file)
    else:
        print("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
