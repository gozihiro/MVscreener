import os
import sys
import pandas as pd
from datetime import datetime, timedelta, timezone
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
import io

# --- ç’°å¢ƒå¤‰æ•°ã®å–å¾— ---
def get_env(name):
    return os.environ.get(name)

CLIENT_ID = get_env('CLIENT_ID')
CLIENT_SECRET = get_env('CLIENT_SECRET')
REFRESH_TOKEN = get_env('REFRESH_TOKEN')
PARENT_FOLDER_ID = get_env('GDRIVE_FOLDER_ID')

# è§£æžå¯¾è±¡ã®å…¨é …ç›®
REQUIRED_COLS = [
    'éŠ˜æŸ„', 'ä¾¡æ ¼', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æˆé•·æ€§åˆ¤å®š', 'å£²ä¸Šæˆé•·(%)', 
    'å–¶æ¥­åˆ©ç›Šæˆé•·(EBITDA)%', 'ç´”åˆ©ç›Šæˆé•·(%)', 'å–¶æ¥­CF(M)', 'æ™‚ä¾¡ç·é¡(B)'
]

def get_drive_service():
    creds = Credentials(token=None, refresh_token=REFRESH_TOKEN, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, token_uri="https://oauth2.googleapis.com/token")
    return build('drive', 'v3', credentials=creds)

def get_or_create_summary_folder(service):
    """ç¾åœ¨ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã«Summaryãƒ•ã‚©ãƒ«ãƒ€ã‚’ç‰¹å®šã¾ãŸã¯ä½œæˆï¼ˆé‡è¤‡é˜²æ­¢ã‚’å¼·åŒ–ï¼‰"""
    # åå‰ã ã‘ã§æ¤œç´¢ã—ã€å¾Œã‹ã‚‰è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨ã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    query = f"name = 'Summary' and '{PARENT_FOLDER_ID}' in parents and trashed = false"
    res = service.files().list(q=query, fields="files(id, mimeType, name)").execute()
    files = res.get('files', [])
    
    # ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹ã‚‚ã®ã‚’æŠ½å‡º
    folders = [f for f in files if f['mimeType'] == 'application/vnd.google-apps.folder']
    
    if folders:
        # æ—¢ã«ã‚ã‚‹å ´åˆã¯ã€æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ•ã‚©ãƒ«ãƒ€ã®IDã‚’è¿”ã™
        return folders[0]['id']
    else:
        # ä¸€åˆ‡å­˜åœ¨ã—ãªã„å ´åˆã®ã¿æ–°è¦ä½œæˆ
        file_metadata = {
            'name': 'Summary',
            'mimeType': 'application/vnd.google-apps.folder',
            'parents': [PARENT_FOLDER_ID]
        }
        folder = service.files().create(body=file_metadata, fields='id').execute()
        print(f"ðŸ“ Summaryãƒ•ã‚©ãƒ«ãƒ€ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")
        return folder.get('id')

def get_target_time_ranges():
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    ranges = []
    for i in range(7):
        day = now - timedelta(days=i)
        if day.weekday() in [1, 2, 3, 4, 5]: # ç«ã€œåœŸ(JST) = æœˆã€œé‡‘(Market)
            start = day.replace(hour=0, minute=0, second=0, microsecond=0)
            end = day.replace(hour=23, minute=59, second=59, microsecond=0)
            ranges.append((start, end))
        if len(ranges) == 5: break
    return sorted(ranges)

def fetch_weekly_data():
    service = get_drive_service()
    ranges = get_target_time_ranges()
    weekly_dfs = []
    market_metadatas = []

    print("=== Phase 1: ãƒ‡ãƒ¼ã‚¿åŽé›†ã¨å¸‚å ´ç’°å¢ƒè§£æž (JST) ===")
    for start, end in ranges:
        market_date = (start - timedelta(days=1)).strftime('%m/%d')
        q = f"'{PARENT_FOLDER_ID}' in parents and name = 'minervini_final_results.csv' and createdTime >= '{start.isoformat()}' and createdTime <= '{end.isoformat()}' and trashed = false"
        res = service.files().list(q=query, fields="files(id, createdTime)", orderBy="createdTime").execute()
        files = res.get('files', [])

        if files:
            file_id = files[0]['id']
            req = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done: _, done = downloader.next_chunk()
            
            fh.seek(0)
            raw_data = fh.read().decode('utf-8-sig').splitlines()
            
            # 1è¡Œç›®ã®å¸‚å ´ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            metadata_line = raw_data[0] if raw_data else "No Metadata"
            market_metadatas.append({'Date': market_date, 'Metadata': metadata_line})
            
            # 2è¡Œç›®ä»¥é™ã®éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(io.StringIO("\n".join(raw_data[1:])))
            for col in REQUIRED_COLS:
                if col not in df.columns: df[col] = "ä¸æ˜Ž"
            
            df = df[REQUIRED_COLS].copy()
            df['Date'] = market_date
            weekly_dfs.append(df)
            print(f"âœ… {market_date}: åŽé›†å®Œäº†")
        else:
            print(f"âŒ {market_date}: æœªæ¤œå‡º")

    return weekly_dfs, market_metadatas

def analyze_detailed_trend(dfs, metadatas):
    if not dfs: return None
    
    print("=== Phase 2: ãƒˆãƒ¬ãƒ³ãƒ‰é›†è¨ˆãƒ•ã‚§ãƒ¼ã‚º ===")
    # éŠ˜æŸ„ã”ã¨ã®å‡ºç¾é »åº¦ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
    all_raw = pd.concat(dfs, ignore_index=True)
    trend_df = all_raw.groupby('éŠ˜æŸ„').size().reset_index(name='å‡ºç¾å›žæ•°')
    
    # å¸‚å ´ç’°å¢ƒè¡Œã®ç®±ã‚’ç”¨æ„
    market_row = {'éŠ˜æŸ„': '### MARKET_ENVIRONMENT ###', 'å‡ºç¾å›žæ•°': '-'}

    for df, meta in zip(dfs, metadatas):
        date = meta['Date']
        # å¸‚å ´ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¾¡æ ¼åˆ—ã®ä½ç½®ã«æŒ¿å…¥
        market_row[f'ä¾¡æ ¼_{date}'] = meta['Metadata']
        
        # éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        daily_data = df.set_index('éŠ˜æŸ„').drop(columns=['Date']).add_suffix(f'_{date}')
        trend_df = trend_df.merge(daily_data, on='éŠ˜æŸ„', how='left')

    # å¸‚å ´ç’°å¢ƒè¡Œã‚’æœ€ä¸Šéƒ¨ã«è¿½åŠ 
    result = pd.concat([pd.DataFrame([market_row]), trend_df], ignore_index=True)

    # ã‚½ãƒ¼ãƒˆå‡¦ç†ï¼ˆæœ€æ–°ã®å£²ä¸Šæˆé•·çŽ‡é †ï¼‰
    latest_date = dfs[-1]['Date'].iloc[0] if isinstance(dfs[-1]['Date'], pd.Series) else dfs[-1]['Date']
    sort_col = f'å£²ä¸Šæˆé•·(%)_{latest_date}'
    
    if sort_col in result.columns:
        result['_sort'] = pd.to_numeric(result[sort_col], errors='coerce').fillna(-999)
        market_header = result.iloc[:1]
        stock_data = result.iloc[1:].sort_values(by=['å‡ºç¾å›žæ•°', '_sort'], ascending=False)
        result = pd.concat([market_header, stock_data]).drop(columns=['_sort'])
    
    return result.fillna('ï¼')

def upload_result_to_drive(file_path):
    service = get_drive_service()
    summary_folder_id = get_or_create_summary_folder(service)
    
    file_name = f"weekly_detailed_trend_{datetime.now().strftime('%Y%m%d')}.csv"
    file_metadata = {'name': file_name, 'parents': [summary_folder_id]}
    media = MediaFileUpload(file_path, mimetype='text/csv')
    
    # Summaryãƒ•ã‚©ãƒ«ãƒ€å†…ã«åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
    query = f"'{summary_folder_id}' in parents and name = '{file_name}' and trashed = false"
    res = service.files().list(q=query).execute()
    files = res.get('files', [])

    if files:
        service.files().update(fileId=files[0]['id'], media_body=media).execute()
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {file_name}")
    else:
        service.files().create(body=file_metadata, media_body=media).execute()
        print(f"âœ… æ–°è¦ä¿å­˜ã—ã¾ã—ãŸ: {file_name}")

if __name__ == "__main__":
    if not all([CLIENT_ID, CLIENT_SECRET, REFRESH_TOKEN, PARENT_FOLDER_ID]):
        print("âŒ èªè¨¼æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        sys.exit(1)

    weekly_data, metadatas = fetch_weekly_data()
    if weekly_data:
        trend_result = analyze_detailed_trend(weekly_data, metadatas)
        if trend_result is not None:
            output_file = "weekly_detailed_trend.csv"
            trend_result.to_csv(output_file, index=False, encoding='utf-8-sig')
            upload_result_to_drive(output_file)
    else:
        print("âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—")
